{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Overlap for 196917 items\n"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "### load the meta data\n",
    "\n",
    "data = []\n",
    "with gzip.open('meta_Electronics.json.gz') as f:\n",
    "        for l in f:\n",
    "            data.append(json.loads(l.strip()))\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df3 = df.fillna('')\n",
    "df4 = df3[df3.title.str.contains('getTime')] # unformatted rows\n",
    "df5 = df3[~df3.title.str.contains('getTime')] # filter those unformatted rows\n",
    "\n",
    "### load the QA\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "qa_df = getDF('qa_Electronics.json.gz')\n",
    "\n",
    "final_df = pd.merge(df5, qa_df, on='asin')\n",
    "print('Overlap for {} items'.format(len(final_df)))\n",
    "final_df = final_df.drop_duplicates(subset=['question'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "23172it [00:02, 9258.44it/s]13\n\n"
    }
   ],
   "source": [
    "only_product_df = final_df.drop_duplicates(subset=['asin'])\n",
    "\n",
    "category_dict = defaultdict(lambda: defaultdict(int))\n",
    "for i, row in tqdm(only_product_df.iterrows()):\n",
    "    category = row['category']\n",
    "    for i, c in enumerate(category):\n",
    "        category_dict[i][c] += 1\n",
    "\n",
    "print(len(category_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "179577it [00:19, 9417.61it/s]\n179577\n179577\n                                     category             assigned_cluster\n0  [Electronics, eBook Readers & Accessories]  eBook Readers & Accessories\n1  [Electronics, eBook Readers & Accessories]  eBook Readers & Accessories\n"
    }
   ],
   "source": [
    "assigned_cluster = []\n",
    "assigned_cluster_level = []\n",
    "for i, row in tqdm(final_df.iterrows()):\n",
    "    category = row['category']\n",
    "    for i, c in enumerate(category):\n",
    "        if category_dict[i][c] < 400:\n",
    "            assigned_cluster.append(c)\n",
    "            assigned_cluster_level.append(i)\n",
    "            break\n",
    "        elif i == len(category)-1:            \n",
    "            assigned_cluster.append(c)\n",
    "            assigned_cluster_level.append(i)\n",
    "\n",
    "print(len(assigned_cluster))\n",
    "print(len(final_df))\n",
    "final_df['assigned_cluster'] = assigned_cluster\n",
    "final_df['assigned_cluster_level'] = assigned_cluster_level\n",
    "print(final_df.head(2)[['category', 'assigned_cluster']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['eBook Readers & Accessories', 'TV Accessories & Parts',\n       'Computer Cable Adapters', 'Tablets', 'Cases', 'Monitors',\n       'Video Surveillance', 'External Components',\n       'MP3 & MP4 Player Accessories', 'Memory Cards',\n       'Batteries, Chargers & Accessories', 'Video Cables', 'USB Cables',\n       'Cleaning & Repair', 'Blank Video Media', 'Audio Cables',\n       'Telephone Accessories', 'Ethernet Cables', 'External Zip Drives',\n       'Power Strips & Surge Protectors', 'Input Devices',\n       'On-Ear Headphones', 'Earbud Headphones', 'CB & Two-Way Radios',\n       'Floppy & Tape Drives', 'Video', 'VCRs', 'Home Theater',\n       'Office Electronics Accessories', 'Binoculars & Scopes',\n       'Cable Security Devices', 'Film Photography',\n       'Over-Ear Headphones', 'Audio & Video Accessories', 'Radios',\n       'Accessories & Supplies', 'Internal Hard Drives', 'Switches',\n       'Security & Surveillance', 'Hubs', 'GPS, Finders & Accessories',\n       'Flashes', 'Routers', 'Cassette Players & Recorders',\n       'SLR Camera Lenses', 'Boomboxes', 'Stereo System Components',\n       'Cord Management', 'Bags & Cases', 'Camera & Photo',\n       'Digital Camera Accessories', 'Remote Controls', 'Distribution',\n       'Headphone Accessories', 'Modem Cables', 'Power Cables',\n       'Cables & Interconnects', 'Computer Headsets',\n       'Filters & Accessories', 'Photo Studio', 'Monitor Accessories',\n       'Fans & Cooling', 'Network Cards', 'Video Projectors',\n       'Camera Lenses', 'Point & Shoot Digital Cameras',\n       'Batteries & Chargers', 'Professional Video Accessories',\n       'Portable CD Players', 'Portable Bluetooth Speakers', 'Antennas',\n       'Blank Media', 'Home Audio', 'Mounts', 'Center-Channel Speakers',\n       'Bookshelf Speakers', 'Turntables & Accessories', 'Mice', 'Modems',\n       'Accessories', 'Remote Controls & Accessories',\n       'Computer Speakers', 'Repeaters', 'Portable Audio & Video',\n       'Subwoofers', 'Keyboards', 'Microcassette Recorders', 'Headphones',\n       'Home Theater Systems', 'Surround Sound Systems',\n       'Telescope & Microscope Accessories', 'Speakers', 'SCSI Cables',\n       'Serial Cables', 'Media Storage & Organization', 'Cables',\n       'I/O Port Cards', 'Satellite TV Equipment',\n       'Internal Power Supplies', 'Firewire Cables', 'Tripods & Monopods',\n       'Computer Microphones', 'Connectors & Adapters', 'Memory',\n       'Satellite Speakers', 'Outdoor Speakers', 'Lens Accessories',\n       'Network Adapters', 'DSLR Cameras', 'Cleaning Equipment',\n       'Camcorder Accessories', 'Memory Card Accessories', 'Backpacks',\n       'PS/2 Cables', 'Light Meters & Accessories',\n       'Uninterruptible Power Supply (UPS)', 'Lenses',\n       'Wireless Access Points', 'Computers & Accessories',\n       'Docking Stations', 'All-in-Ones', 'DVD-VCR Combos',\n       'Internal Optical Drives', 'Chargers & Adapters',\n       'Vehicle Electronics Accessories', 'Ceiling & In-Wall Speakers',\n       'Marine Electronics', 'Car Stereo Receivers', 'Changers',\n       'Lighting', 'Darkroom Supplies', 'Video Converters',\n       'Tripod & Monopod Accessories', 'Flash Accessories',\n       'Floorstanding Speakers', 'Data Storage', 'Projectors',\n       'DVD Players & Recorders', 'Towers', 'Traditional Laptops',\n       'Network Transceivers', 'Cooling Pads & External Fans',\n       'Laptop Accessories', 'Speaker Parts & Components',\n       'Print Servers', 'Car Video', 'Car Audio',\n       'Compact Radios & Stereos', 'Amplifiers', 'Projection Screens',\n       'VGA Cables', 'Keyboard & Mouse Combos', 'Equalizers',\n       'Satellite Television', 'Binocular Accessories', 'Crossover Parts',\n       'Webcams', 'Internal Modems', 'USB Flash Drives', 'Motherboards',\n       'Bags, Cases & Sleeves', 'LED & LCD TVs', 'Screen Protectors',\n       'Styluses', 'Internal TV Tuner & Capture Cards', 'Graphics Cards',\n       'Satellite Radio', 'Minis', '3D Glasses', 'Audio Docks',\n       'External Hard Drives', 'MP3 & MP4 Players', 'Network Antennas',\n       'Computers & Tablets', 'USB Gadgets', 'Digital Voice Recorders',\n       'Keyboard & Mice Accessories', 'Laptop Replacement Parts',\n       'Networking Products', 'CPU Processors', 'Car Safety & Security',\n       'Portable DVD Players', 'Streaming Media Players',\n       'Portable Line-In Speakers', 'Printers & Scanners',\n       'Blu-ray Players & Recorders', 'Batteries', 'Televisions',\n       'Internal Sound Cards', 'Stands', 'HD DVD Players',\n       'Hard Drive Accessories', 'USB Hubs', 'Car & Vehicle Electronics',\n       'Messenger Bags', 'Camcorder Lenses', 'Car Electronics',\n       'Wireless & Streaming Audio', 'Internal Components', 'Briefcases',\n       'SATA Cables', 'Digital Cameras', 'Minidisc Players', 'Sleeves',\n       'Computer Accessories & Peripherals', 'Plastic', 'Shoulder Bags',\n       'Folio Cases', 'Rain Covers', 'Underwater Photography', 'Desktops',\n       'Replacement Screens', 'Analog-to-Digital (DTV) Converters',\n       'Servers', 'Television & Video', 'Aviation Electronics',\n       'Network Attached Storage', 'Sound Bars', 'Viewfinders', 'Laptops',\n       'TV-DVD Combos', 'Security Locks', 'Portable & Handheld TVs',\n       'AV Receivers & Amplifiers', 'Mirrorless Camera Lenses',\n       'Warranties & Services', 'Electronics Warranties',\n       'Keyboards, Mice & Accessories', 'Skins & Decals',\n       'Portable Speakers & Docks', 'Mirrorless Cameras',\n       'Tablet Accessories', 'Computer Components', 'Screen Filters',\n       'Service Plans', '', 'Accessory Bundles',\n       'Internal Solid State Drives', 'Cables & Cords',\n       'Slick protective case in black for the Apple iPad',\n       'Keyboard Cases', 'Thunderbolt Cables',\n       'External Solid State Drives', 'Video Studio',\n       'Modem Router Combos', 'Desktop Barebones',\n       'Vehicle Tracking and Monitoring Modules',\n       'Tablet Replacement Parts', 'Parallel Cables', '2 in 1 Laptops',\n       'Bags', 'Lightning Cables', 'Wearable Technology', 'Imported',\n       'Single Board Computers',\n       'Ultra Small Form Factor (USFF) Computing Platform at approximately 4\" x 4\"',\n       'Internal Memory Card Readers',\n       'UNIVERSAL FIT technology gives you the flexibility to use one case for countless brands and models of tablets.',\n       'Speaker Repair', 'Made in China', 'Silicone',\n       'Whole Home & Mesh Wi-Fi Systems', 'OLED TVs',\n       'Camera & Camcorder Lens Bundles', 'Home Audio Crossovers & Parts',\n       'Lighting & Studio', 'Television Accessories', 'ABS TPR PC',\n       'Digital Media Receivers'], dtype=object)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "final_df['assigned_cluster'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index(['category', 'description', 'title', 'image', 'brand', 'feature', 'rank',\n       'main_cat', 'date', 'price', 'asin', 'also_buy', 'also_view',\n       'similar_item', 'tech1', 'tech2', 'details', 'fit', 'questionType',\n       'answerTime', 'unixTime', 'question', 'answerType', 'answer',\n       'assigned_cluster', 'assigned_cluster_level'],\n      dtype='object')"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/t-bmajum/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n 44%|████▎     | 85895/196917 [13:10<14:47, 125.14it/s]"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-817f9eb5a4c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mno_kw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkw_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mkeywords_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/yake/yake.py\u001b[0m in \u001b[0;36mextract_keywords\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mdc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataCore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopword_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopword_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindowsSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindowsSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mdc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_single_terms_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mdc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_mult_terms_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mresultSet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/yake/datarepresentation.py\u001b[0m in \u001b[0;36mbuild_single_terms_features\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mavgTF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidTFs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mstdTF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidTFs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mmaxTF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateH\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxTF\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxTF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavgTF\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mavgTF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdTF\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdTF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "VERB_TAGS = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "\n",
    "index = 1 # change this for other questions \n",
    "q = final_df['question'].iloc(index)\n",
    "product_title = final_df['title'].iloc(index)\n",
    "product_title = final_df['assigned_cluster'].iloc(index)\n",
    "\n",
    "keywords = kw_extractor.extract_keywords(q)\n",
    "\n",
    "for kw in keywords:\n",
    "    \n",
    "\n",
    "tokenized_q = word_tokenize(q)\n",
    "pos_tags = nltk.pos_tag(tokenized_q)\n",
    "\n",
    "print('Title: ' + product_title + '\\n')\n",
    "print('Question: ' + q + '\\n')\n",
    "print('Keywords: ' + keywords_list + '\\n')\n",
    "print('POS tags: ' + pos_tags + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "('open office wide', 0.00010860644121698296)\n('glossy imr pearl', 0.00010860644121698296)\n('imr pearl white', 0.00010860644121698296)\n('camera zbd guaranteed', 0.00010860644121698296)\n('quality lcd store', 0.00014985524694779634)\n('office wide multi-touchpad', 0.00017963776274479537)\n('pearl white color', 0.00017963776274479537)\n('including open office', 0.0001796377627447954)\n('convenient glossy imr', 0.0001796377627447954)\n('built-in camera zbd', 0.0001796377627447954)\n('shock-proof design wireless', 0.00019891072292280027)\n('applications including open', 0.00023846771841820355)\n('internet storage protect', 0.00023846771841820355)\n('connectivity save time', 0.00030535855075990463)\n('boot time preloaded', 0.00030535855075990463)\n('hour battery life', 0.0003949446519670444)\n('sec boot time', 0.0004055412934082321)\n('day computing', 0.0018959639534652487)\n('lcd store', 0.0018959639534652487)\n('easy to learn', 0.0020748961992834336)\n"
    }
   ],
   "source": [
    "text = \"Easy to Learn, Work, and Play Easy to use with one click interface 7.5 hour battery life allows for \\\"One Day Computing\\\" Travel light, weighting only 2.94lbs Drop tested, Shock-Proof design Wireless 802.11b/g/n connectivity Save time with fast 15 sec boot time Preloaded with over 50 applications including Open Office Wide Multi-Touchpad for comfort and convenient Glossy IMR Pearl White color for elegant look Connect with friend with built-in Camera ZBD Guaranteed for best quality LCD Store & Share your files with 20G internet storage Protect your investment with sleeve case\"\n",
    "\n",
    "import yake\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "keywords = kw_extractor.extract_keywords(text)\n",
    "for kw in keywords:\n",
    "    print(kw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Does                                      \n  ___|___________                              \n |             cable                          \n |    ___________|____________________         \n |   |     |         beside           |       \n |   |     |           |              |        \n |   |     |          work            |       \n |   |     |      _____|_____         |        \n |   |     |     |          pin     charge    \n |   |     |     |           |    ____|_____   \n ?  any  other  the          30  to        HD+\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[None]"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "# !pip install spacy\n",
    "import spacy\n",
    "from nltk import Tree\n",
    "# !python -m spacy download en_core_web_lg\n",
    "\n",
    "en_nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "doc = en_nlp(\"Does any other cable beside the 30 pin work to charge HD+?\")\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_\n",
    "\n",
    "\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Use device: gpu\n---\nLoading: tokenize\nWith settings: \n{'model_path': '/home/t-bmajum/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n---\nLoading: pos\nWith settings: \n{'model_path': '/home/t-bmajum/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/home/t-bmajum/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n---\nLoading: lemma\nWith settings: \n{'model_path': '/home/t-bmajum/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\nBuilding an attentional Seq2Seq model...\nUsing a Bi-LSTM encoder\nUsing soft attention for LSTM.\nFinetune all embeddings.\n[Running seq2seq lemmatizer with edit classifier]\n---\nLoading: depparse\nWith settings: \n{'model_path': '/home/t-bmajum/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/home/t-bmajum/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\nDone loading processors!\n---\n<Token index=1;words=[<Word index=1;text=Is;lemma=be;upos=AUX;xpos=VBZ;feats=Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin;governor=3;dependency_relation=cop>]>\n<Token index=2;words=[<Word index=2;text=it;lemma=it;upos=PRON;xpos=PRP;feats=Case=Nom|Gender=Neut|Number=Sing|Person=3|PronType=Prs;governor=3;dependency_relation=expl>]>\n<Token index=3;words=[<Word index=3;text=possible;lemma=possible;upos=ADJ;xpos=JJ;feats=Degree=Pos;governor=0;dependency_relation=root>]>\n<Token index=4;words=[<Word index=4;text=to;lemma=to;upos=PART;xpos=TO;feats=_;governor=5;dependency_relation=mark>]>\n<Token index=5;words=[<Word index=5;text=read;lemma=read;upos=VERB;xpos=VB;feats=VerbForm=Inf;governor=3;dependency_relation=csubj>]>\n<Token index=6;words=[<Word index=6;text=using;lemma=use;upos=VERB;xpos=VBG;feats=VerbForm=Ger;governor=5;dependency_relation=xcomp>]>\n<Token index=7;words=[<Word index=7;text=this;lemma=this;upos=DET;xpos=DT;feats=Number=Sing|PronType=Dem;governor=8;dependency_relation=det>]>\n<Token index=8;words=[<Word index=8;text=product;lemma=product;upos=NOUN;xpos=NN;feats=Number=Sing;governor=6;dependency_relation=obj>]>\n<Token index=9;words=[<Word index=9;text=at;lemma=at;upos=ADP;xpos=IN;feats=_;governor=10;dependency_relation=case>]>\n<Token index=10;words=[<Word index=10;text=night;lemma=night;upos=NOUN;xpos=NN;feats=Number=Sing;governor=6;dependency_relation=obl>]>\n<Token index=11;words=[<Word index=11;text=?;lemma=?;upos=PUNCT;xpos=.;feats=_;governor=3;dependency_relation=punct>]>\n('Is', '3', 'cop')\n('it', '3', 'expl')\n('possible', '0', 'root')\n('to', '5', 'mark')\n('read', '3', 'csubj')\n('using', '5', 'xcomp')\n('this', '8', 'det')\n('product', '6', 'obj')\n('at', '10', 'case')\n('night', '6', 'obl')\n('?', '3', 'punct')\nIs it possible to read using this product at night?\n"
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "# stanfordnlp.download('en')  \n",
    "nlp = stanfordnlp.Pipeline()\n",
    "doc = nlp(\"Is it possible to read using this product at night?\")\n",
    "a  = doc.sentences[0]\n",
    "dir(a)\n",
    "a.print_tokens()\n",
    "a.print_dependencies()\n",
    "\n",
    "print('Is it possible to read using this product at night?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'VBZ'"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "dir(a)\n",
    "a.tokens[0].words[0].xpos\n",
    "\n",
    "# b = a.print_dependencies()\n",
    "# for dep_edge in a.dependencies:\n",
    "#     print((int(dep_edge[2].index), int(dep_edge[0].index), dep_edge[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "possible -> Is | Relation cop\npossible -> it | Relation expl\nROOT -> possible | Relation root\nread -> to | Relation mark\npossible -> read | Relation csubj\nread -> using | Relation xcomp\nproduct -> this | Relation det\nusing -> product | Relation obj\nnight -> at | Relation case\nusing -> night | Relation obl\npossible -> ? | Relation punct\n"
    }
   ],
   "source": [
    "for dep_edge in a.dependencies:\n",
    "        print('{} -> {} | Relation {}'.format(dep_edge[0].text, dep_edge[2].text, dep_edge[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Use device: gpu\n---\nLoading: tokenize\nWith settings: \n{'model_path': '/home/t-bmajum/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n---\nLoading: pos\nWith settings: \n{'model_path': '/home/t-bmajum/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/home/t-bmajum/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n---\nLoading: lemma\nWith settings: \n{'model_path': '/home/t-bmajum/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\nBuilding an attentional Seq2Seq model...\nUsing a Bi-LSTM encoder\nUsing soft attention for LSTM.\nFinetune all embeddings.\n[Running seq2seq lemmatizer with edit classifier]\n---\nLoading: depparse\nWith settings: \n{'model_path': '/home/t-bmajum/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/home/t-bmajum/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\nDone loading processors!\n---\n"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import yake\n",
    "import stanfordnlp\n",
    "import itertools\n",
    "\n",
    "nlp_pipeline = stanfordnlp.Pipeline()\n",
    "kw_extractor = yake.KeywordExtractor(n=2)\n",
    "\n",
    "def build_schema(text, nlp_pipeline, kw_extractor):\n",
    "\n",
    "    # run a sent tokenizer\n",
    "\n",
    "    # run the following for each sent\n",
    "\n",
    "    # VERB_TAGS = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    VERB_TAGS = ['VB', 'VBG', 'VBZ']\n",
    "\n",
    "    # run StanfordNLP pipeline\n",
    "    doc = nlp_pipeline(text)\n",
    "    sent  = doc.sentences[0]\n",
    "\n",
    "    # all tokens + root\n",
    "    tokens = ['root']\n",
    "    tokens += [t.words[0].text.lower() for t in sent.tokens]\n",
    "\n",
    "    # obtain all verbs and their indices\n",
    "    verbs = []\n",
    "    verb_indices = []\n",
    "    for t in sent.tokens:\n",
    "        if t.words[0].xpos in VERB_TAGS:\n",
    "            verbs.append(t.words[0].text)\n",
    "            verb_indices.append(int(t.words[0].index))\n",
    "\n",
    "    # remove all verbs from keywords\n",
    "    try:\n",
    "        keywords = kw_extractor.extract_keywords(text)\n",
    "    except Exception:\n",
    "        keywords = []\n",
    "\n",
    "    unigram_keywords_map = {}\n",
    "    for kw in keywords:\n",
    "        unigram_keywords_map[kw[0].lower()] = kw[0].lower().split()\n",
    "    \n",
    "    unigram_keywords = list(itertools.chain(*list(unigram_keywords_map.values())))\n",
    "\n",
    "    # after this everything will be unigrams\n",
    "\n",
    "    keywords_wo_verbs = [kw for kw in unigram_keywords if kw not in verbs]\n",
    "\n",
    "    # obtain all keyword indices\n",
    "    keyword_indices = [tokens.index(kw) for kw in keywords_wo_verbs if kw in tokens]\n",
    "\n",
    "    # initialize dependency tree\n",
    "    G = nx.DiGraph()\n",
    "    for dep_edge in sent.dependencies:\n",
    "        G.add_edge(int(dep_edge[0].index), int(dep_edge[2].index), relation=dep_edge[1])\n",
    "\n",
    "    relation_edge_dict = nx.get_edge_attributes(G,'relation')\n",
    "\n",
    "    schema = {}\n",
    "\n",
    "    tuple_schema = []\n",
    "\n",
    "    for v in verb_indices:\n",
    "        # find a path from verb v to each keywords\n",
    "        for k in keyword_indices:\n",
    "            # we are finding shortest paths\n",
    "            try:\n",
    "                path = nx.shortest_path(G, source=v, target=k)\n",
    "            except:\n",
    "                # print('No path obtained')\n",
    "                continue\n",
    "                # check is path contains more than 2 nodes\n",
    "            if len(path) > 2:\n",
    "                # walk backward from the target to source\n",
    "                for i, node in reversed(list(enumerate(path))):\n",
    "                    if node in verb_indices:\n",
    "                        # retrieve the first parent verb of the keyword\n",
    "                        # obtain the relation with its child on the path\n",
    "                        tuple_schema.append((tokens[v], tokens[k], relation_edge_dict[(node, path[i+1])], len(path)))\n",
    "                        break\n",
    "            else:\n",
    "                # default case for an one-hop path between verb and keyword\n",
    "                tuple_schema.append((tokens[v], tokens[k], relation_edge_dict[(v, k)], 2))\n",
    "\n",
    "    # retain only the closest verb for each keyword\n",
    "    for kw in keywords_wo_verbs:\n",
    "        kw_tuples = [t for t in tuple_schema if t[1] == kw]\n",
    "        if kw_tuples:\n",
    "            final_tuple = min(kw_tuples, key = lambda t: t[1])  \n",
    "            schema[kw] = final_tuple[:-1] # drop the path length\n",
    "        else:\n",
    "            schema[kw] = kw\n",
    "\n",
    "    merged_schema = {}\n",
    "    captured_uni_kw = []\n",
    "    for kw, uni_kw in unigram_keywords_map.items():\n",
    "        if uni_kw[0] in keywords_wo_verbs:\n",
    "            if uni_kw[0] not in captured_uni_kw:\n",
    "                # collect tuples based on the first unigram entry\n",
    "                merged_schema[kw] = schema[uni_kw[0]]\n",
    "                captured_uni_kw.extend(uni_kw)\n",
    "    \n",
    "    for kw, t in merged_schema.items():\n",
    "        if isinstance(t, tuple):  \n",
    "            merged_schema[kw] = (t[0], kw, t[-1])\n",
    "        else:\n",
    "            merged_schema[kw] = kw\n",
    "\n",
    "    merged_schema = list(merged_schema.values())\n",
    "\n",
    "    # default case, no schema, hence keep all keywords including verbs\n",
    "    if len(tuple_schema) == 0:\n",
    "        merged_schema = [kw[0].lower() for kw in keywords]\n",
    "\n",
    "    merged_schema = list(set(merged_schema))\n",
    "\n",
    "    return merged_schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('read', 'product', 'obj'), ('read', 'graphical interface', 'obj')]"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "text = 'Is it possible to read using this product using graphical interface?'\n",
    "build_schema(text, nlp_pipeline, kw_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['nook color',\n 'dessin cover',\n 'noir cover',\n 'tablet',\n 'standard nook',\n 'e-reader',\n 'style',\n 'covers',\n 'protect',\n 'leather']"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# build schema for product descriptions\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor(n=2)\n",
    "\n",
    "def build_schema_from_desc(text, kw_extractor):\n",
    "    # remove all verbs from keywords\n",
    "    try:\n",
    "        keywords = kw_extractor.extract_keywords(text)\n",
    "    except Exception:\n",
    "        keywords = []\n",
    "\n",
    "    unigram_keywords_map = {}\n",
    "    for kw in keywords:\n",
    "        unigram_keywords_map[kw[0].lower()] = kw[0].lower().split()\n",
    "\n",
    "    merged_schema = []\n",
    "\n",
    "    captured_uni_kw = []\n",
    "    for kw, uni_kw in unigram_keywords_map.items():\n",
    "        if uni_kw[0] not in captured_uni_kw:\n",
    "            # collect tuples based on the first unigram entry\n",
    "            merged_schema.append(kw)\n",
    "            captured_uni_kw.extend(uni_kw)\n",
    "    \n",
    "    return merged_schema\n",
    "    \n",
    "text = ' '.join(final_df.iloc[1]['description'])\n",
    "\n",
    "build_schema_from_desc(text, kw_extractor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Brand Name', 'Item Weight', 'Product Dimensions']"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# get table schema\n",
    "\n",
    "def build_schema_from_table(table):\n",
    "    if table:\n",
    "        return list(table.keys())\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "build_schema_from_table(final_df.iloc[1]['tech1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# text = 'Is it possible to read using this product at night?'\n",
    "\n",
    "schemas = []\n",
    "\n",
    "for i in range(100):\n",
    "    text = final_df['question'].iloc[i].lower()\n",
    "    schema = build_schema(text, nlp_pipeline, kw_extractor)\n",
    "\n",
    "    schemas.append(\n",
    "        {'question': text,\n",
    "        'schema': schema}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "26\n"
    }
   ],
   "source": [
    "# save with schema\n",
    "\n",
    "import json\n",
    "prod_dict = {}\n",
    "cat = \"Laptops\"\n",
    "cat_df = final_df[final_df['assigned_cluster'] == cat]\n",
    "grps = cat_df.groupby(cat_df['asin'])\n",
    "for name, g in grps:\n",
    "    item_dict = {}\n",
    "    qa = []\n",
    "    item_id = g['asin'].iloc[0]\n",
    "    for i, r in g.iterrows():\n",
    "        item_dict['title'] = r['title']\n",
    "        # item_dict['description'] = r['description']\n",
    "        item_dict['description_schema'] = build_schema_from_desc(' '.join(r['description']), kw_extractor)\n",
    "        # item_dict['table1'] = r['tech1']\n",
    "        # item_dict['table2'] = r['tech2']\n",
    "        table_schema = []\n",
    "        table_schema.extend(build_schema_from_table(r['tech1']))\n",
    "        table_schema.extend(build_schema_from_table(r['tech2']))\n",
    "        item_dict['table_schema'] = table_schema\n",
    "        schema = build_schema(r['question'].lower(), nlp_pipeline, kw_extractor)\n",
    "        qa.append({'question': r['question'], 'schema': schema})\n",
    "    item_dict['questions'] = qa\n",
    "    prod_dict[item_id] = item_dict\n",
    "\n",
    "print(len(prod_dict))\n",
    "\n",
    "with open('{}_schema.json'.format(cat), 'w') as fp:\n",
    "    json.dump(prod_dict, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "dict_keys(['B00191QN6O', 'B001BY97IU', 'B001BY97JO', 'B001BYB5ZS', 'B001BYB620', 'B001FWXCFM', 'B001GCUOFC', 'B001GIPSAM', 'B001GNBD8I', 'B0021AG0RY', 'B00284CBKS', 'B0029PQFT4', 'B002DYIXMS', 'B002MUCC52', 'B002P3KMXA', 'B0030LQ438', 'B00322PYUY', 'B003JZC5NI', 'B003TPKDY6', 'B0041G5XFQ', 'B0042TS7GE', 'B004G8QZQK', 'B0092IBD8Y', 'B00CTHQORA', 'B00FK0BOUU', 'B00HIY9I7C'])"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# grouping to obtain global schema\n",
    "\n",
    "# creating a simple product schema object\n",
    "# key: product id, value: schema\n",
    "\n",
    "simple_prod_dict = {}\n",
    "for p, s in prod_dict.items():\n",
    "    local_schema = []\n",
    "    local_schema.extend(s['description_schema'])\n",
    "    local_schema.extend(s['table_schema'])\n",
    "    for q in s['questions']:\n",
    "        local_schema.extend(q['schema'])\n",
    "    simple_prod_dict[p] = local_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping based on keywords\n",
    "# then bigram\n",
    "# then unigram\n",
    "\n",
    "# take out good bigram\n",
    "# for each bad bigram, tokenize each bigram, and see an unigram cluster to merge\n",
    "\n",
    "corpus = list(itertools.chain.from_iterable(list(simple_prod_dict.values())))\n",
    "only_keywords = [t[1].lower() if isinstance(t, tuple) else t.lower() for t in corpus]\n",
    "print(len(set(only_keywords)))\n",
    "\n",
    "vocabulary = defaultdict(int)\n",
    "for t in only_keywords:\n",
    "    vocabulary[t] += 1\n",
    "\n",
    "sorted_vocab = sorted(vocabulary.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "import collections\n",
    "\n",
    "vocabulary = collections.OrderedDict(sorted_vocab)\n",
    "\n",
    "# for k, v in vocabulary.items():\n",
    "#     if len(k.split()) > 1:\n",
    "#         print(k, v)\n",
    "\n",
    "bigram_vocab = defaultdict(int)\n",
    "unigram_vocab = defaultdict(int)\n",
    "for k, v in vocabulary.items():\n",
    "    if len(k.split()) == 2:\n",
    "        bigram_vocab[k] = v\n",
    "    if len(k.split()) == 1:\n",
    "        unigram_vocab[k] = v\n",
    "\n",
    "print(len(bigram_vocab))\n",
    "print(len(unigram_vocab))\n",
    "\n",
    "print(bigram_vocab)\n",
    "\n",
    "counter = 0\n",
    "for k, v in bigram_vocab.items():\n",
    "    if v < 3:\n",
    "        token_0, token_1 = k.split()\n",
    "        value_0, value_1 = unigram_vocab[token_0], unigram_vocab[token_1]\n",
    "        if value_0 + value_1 != 0:\n",
    "            if value_1 >= value_0:\n",
    "                unigram_vocab[token_1] += 1\n",
    "                counter += 1 \n",
    "            else:\n",
    "                unigram_vocab[token_0] += 1\n",
    "                counter += 1\n",
    "\n",
    "\n",
    "unigram_vocab\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "prod_dict = {}\n",
    "cat = \"Cameras\"\n",
    "cat_df = final_df[final_df['assigned_cluster'] == cat]\n",
    "grps = cat_df.groupby(cat_df['asin'])\n",
    "for name, g in grps:\n",
    "    item_dict = {}\n",
    "    qa = []\n",
    "    item_id = g['asin'].iloc[0]\n",
    "    for i, r in g.iterrows():\n",
    "        item_dict['title'] = r['title']\n",
    "        item_dict['description'] = r['description']\n",
    "        item_dict['table1'] = r['tech1']\n",
    "        item_dict['table2'] = r['tech2']\n",
    "        schema = build_schema(r['question'].lower(), nlp_pipeline, kw_extractor)\n",
    "        qa.append({'question': r['question'], 'schema': schema, 'answer': r['answer']})\n",
    "    item_dict['qa'] = qa\n",
    "    prod_dict[item_id] = item_dict\n",
    "\n",
    "with open('{}_schema.json'.format(cat), 'w') as fp:\n",
    "    json.dump(prod_dict, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "prod_dict = {}\n",
    "# cat = \"Cameras\"\n",
    "# cat_df = final_df[final_df['assigned_cluster'] == cat]\n",
    "grps = final_df.groupby(final_df['asin'])\n",
    "\n",
    "sampled_products = random.sample(list(final_df['asin']), 1000)\n",
    "\n",
    "for name, g in grps:\n",
    "    if g['asin'].iloc[0] not in sampled_products:\n",
    "        continue\n",
    "    else:\n",
    "        item_dict = {}\n",
    "        qa = []\n",
    "        item_id = g['asin'].iloc[0]\n",
    "        for i, r in g.iterrows():\n",
    "            item_dict['title'] = r['title']\n",
    "            item_dict['category'] = r['assigned_cluster']\n",
    "            item_dict['description'] = r['description']\n",
    "            item_dict['table1'] = r['tech1']\n",
    "            item_dict['table2'] = r['tech2']\n",
    "            # schema = build_schema(r['question'].lower(), nlp_pipeline, kw_extractor)\n",
    "            qa.append({'question': r['question']})\n",
    "        item_dict['questions'] = qa\n",
    "        prod_dict[item_id] = item_dict\n",
    "\n",
    "with open('{}.json'.format('Sampled_1000'), 'w') as fp:\n",
    "    json.dump(prod_dict, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[5, 6, 10]"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "nx.shortest_path(G, source=5, target=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/t-bmajum/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('Is', 'VBZ'),\n ('this', 'DT'),\n ('cover', 'NN'),\n ('the', 'DT'),\n ('one', 'NN'),\n ('that', 'WDT'),\n ('fits', 'VBZ'),\n ('the', 'DT'),\n ('old', 'JJ'),\n ('nook', 'NN'),\n ('color', 'NN'),\n ('?', '.'),\n ('Which', 'NNP'),\n ('I', 'PRP'),\n ('believe', 'VBP'),\n ('is', 'VBZ'),\n ('8x5', 'CD'),\n ('.', '.')]"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = word_tokenize(final_df.iloc[0]['question'])\n",
    "nltk.pos_tag(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "\n",
    "masked_question = []\n",
    "for q in fina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "('work with sony', 0.020435055290363522)\n('card slot', 0.02966460909236746)\n('read the sdhc', 0.033892441937102495)\n('macbook pro', 0.033892441937102495)\n('sdhc card', 0.04949487345881267)\n('sony', 0.11060549338282699)\n('slot', 0.11060549338282699)\n('card', 0.13023752997463905)\n('work', 0.18105634546484617)\n('read', 0.18105634546484617)\n('sdhc', 0.18105634546484617)\n('macbook', 0.18105634546484617)\n('pro', 0.18105634546484617)\n"
    }
   ],
   "source": [
    "text = \"Does this work with sony EX1 and can i read the sdhc card on my macbook pro through the sd card slot?\"\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "keywords = kw_extractor.extract_keywords(text)\n",
    "\n",
    "for kw in keywords:\n",
    "    print(kw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use YAKE\n",
    "# use POS taggging and remove verbs\n",
    "# add any additional NN, NNP, NNS"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}